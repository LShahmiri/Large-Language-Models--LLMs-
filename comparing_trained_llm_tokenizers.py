# -*- coding: utf-8 -*-
"""Comparing_Trained_LLM_Tokenizers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LZ0Qbs1fapQ8zJBwTNzg8OgsME2vK9E3
"""

from transformers import AutoModelForCausalLM, AutoTokenizer

colors_list = [
    '102;194;165', '252;141;98', '141;160;203',
    '231;138;195', '166;216;84', '255;217;47'
]

def show_tokens(sentence, tokenizer_name):
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    token_ids = tokenizer(sentence).input_ids
    for idx, t in enumerate(token_ids):
        print(
            f'\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +
            tokenizer.decode(t) +
            '\x1b[0m',
            end=' '
        )

text = """
English and CAPITALIZATION
üéµ È∏ü
show_tokens False None elif == >= else: two tabs:"    " Three tabs: "       "
12.0*50=600
"""

#BERT base model(uncased)(2018)
show_tokens(text, "bert-base-uncased")
'''
Tokenization method: WordPiece
The newline breaks are gone, which makes the model blind to information encoded in newlines (e.g., a chat log when each turn is in a new line).

All the text is in lowercase.

The word ‚Äúcapitalization‚Äù is encoded as two subtokens: capital ##ization. The ## characters are used to indicate this token is a partial token connected to the token that precedes it. This is also a method to indicate where the spaces are, as it is assumed tokens without ## in front have a space before them.

The emoji and Chinese characters are gone and replaced with the [UNK] special token indicating an ‚Äúunknown token.‚Äù
'''

#BERT base model(cased)(2018)
show_tokens(text, "bert-base-cased")
'''
Tokenization method: WordPiece
The cased version of the BERT tokenizer differs mainly in including uppercase tokens.

Notice how ‚ÄúCAPITALIZATION‚Äù is now represented as eight tokens: CA ##PI ##TA ##L ##I ##Z ##AT ##ION.

Both BERT tokenizers wrap the input within a starting [CLS] token and a closing [SEP] token. [CLS] and [SEP] are utility tokens used to wrap the input text and they serve their own purposes. [CLS] stands for classification as it‚Äôs a token used at times for sentence classification. [SEP] stands for separator, as it‚Äôs used to separate sentences in some applications that require passing two sentences to a model
'''

#GPT-2 (2019)
show_tokens(text, "gpt2")
'''
Tokenization method: Byte pair encoding (BPE)
With the GPT-2 tokenizer, we notice the following:

The newline breaks are represented in the tokenizer.

Capitalization is preserved, and the word ‚ÄúCAPITALIZATION‚Äù is represented in four tokens.

The üéµÈ∏ü characters are now represented by multiple tokens each. While we see these tokens printed as the ÔøΩ character, they actually stand for different tokens. For example, the üéµ emoji is broken down into the tokens with token IDs 8582, 236, and 113. The tokenizer is successful in reconstructing the original character from these tokens. We can see that by printing tokenizer.decode([8582, 236, 113]), which prints out üéµ.

The two tabs are represented as two tokens (token number 197 in that vocabulary) and the four spaces are represented as three tokens (number 220) with the final space being a part of the token for the closing quote character.

The two tabs are represented as two tokens (token number 197 in that vocabulary) and the four spaces are represented as three tokens (number 220) with the final space being a part of the token for the closing quote character.

Note
'''

#Flan-T5 (2022)
show_tokens(text, "google/flan-t5-small")
'''
The Flan-T5 family of models use  the SentencePiece method. We notice the following:

No newline or whitespace tokens; this would make it challenging for the model to work with code.

The emoji and Chinese characters are both replaced by the <unk> token, making the model completely blind to them.
'''

# The official is `tiktoken` but this the same tokenizer on the HF platform
show_tokens(text, "Xenova/gpt-4")
'''
Tokenization method: BPE

Vocabulary size: A little over 100,000

Special tokens:

<|endoftext|>

Fill in the middle tokens. These three tokens enable the LLM to generate a completion given not only the text before it but also considering the text after it
The GPT-4 tokenizer behaves similarly to its ancestor, the GPT-2 tokenizer. Some differences are:

The GPT-4 tokenizer represents the four spaces as a single token. In fact, it has a specific token for every sequence of whitespaces up to a list of 83 whitespaces.

The Python keyword elif has its own token in GPT-4. Both this and the previous point stem from the model‚Äôs focus on code in addition to natural language.

The GPT-4 tokenizer uses fewer tokens to represent most words. Examples here include ‚ÄúCAPITALIZATION‚Äù (two tokens versus four) and ‚Äútokens‚Äù (one token versus three).

Refer back to what we said about the GPT-2 tokenizer with regards to the ≈Å tokens.
'''

# You need to request access before being able to use this tokenizer
#StarCoder2 (2024)
show_tokens(text, "bigcode/starcoder2-15b")
'''
Tokenization method: Byte pair encoding (BPE)

This is an encoder that focuses on code generation:

Similar to GPT-4, it encodes the list of whitespaces as a single token.

A major difference here to everything we‚Äôve seen so far is that each digit is assigned its own token (so 600 becomes 6 0 0). The hypothesis here is that this would lead to better representation of numbers and mathematics. In GPT-2, for example, the number 870 is represented as a single token. But 871 is represented as two tokens (8 and 71). You can intuitively see how that might be confusing to the model and how it represents numbers.
'''

#Galactica

show_tokens(text, "facebook/galactica-1.3b")
'''
Tokenization method: Byte pair encoding (BPE)

The Galactica tokenizer behaves similar to StarCoder2 in that it has code in mind. It also encodes whitespaces in the same way: assigning a single token to sequences of whitespace of different lengths. It differs in that it also does that for tabs, though. So from all the tokenizers we‚Äôve seen so far, it‚Äôs the only one that assigns a single token to the string made up of two tabs ('\t\t').
'''

#Phi-3 (and Llama 2)

show_tokens(text, "microsoft/Phi-3-mini-4k-instruct")
'''Tokenization method: Byte pair encoding (BPE)
'''